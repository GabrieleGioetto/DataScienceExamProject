{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df = pd.read_csv(\"competition_dataset/dev.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(df[pd.isna(df[\"region_1\"]) & pd.isna(df[\"region_2\"])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "print(df[\"designation\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    print(df[\"designation\"].value_counts()[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#df[df[\"designation_tr\"] == \"reserva\" | df[\"designation_tr\"] == \"riserva\" | df[\"designation_tr\"] == \"rèserve\"][\"designation_tr\"] = \"reserve\"\n",
    "df[\"designation_tr\"] = df[\"designation\"].apply(lambda d: d.lower() if pd.isna(d) == False else d)\n",
    "df[\"designation_tr\"] = df[\"designation_tr\"].replace([\"reserva\",\"riserva\",\"rèserve\"],[\"reserve\",\"reserve\",\"reserve\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rimuovo regioni doppie e creo regione unica\n",
    "df.loc[df[\"region_1\"] == df[\"region_2\"], \"region_2\"] = \"\"\n",
    "df[\"region_complete\"] = df[[\"region_1\",\"region_2\"]].fillna(\" \").agg(' '.join, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "singular_region_complete = set(df[\"region_complete\"])\n",
    "len(singular_region_complete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import geocoder\n",
    "\n",
    "g = geocoder.arcgis('Redlands, CA')\n",
    "print(g.latlng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "import geocoder\n",
    "\n",
    "singular_region_complete_with_latlong = []\n",
    "for region in singular_region_complete:\n",
    "    print(region)\n",
    "    loc =  geocoder.arcgis(region,maxRows=1)\n",
    "    print(loc)\n",
    "    try:\n",
    "        singular_region_complete_with_latlong.append((region, loc.latlng[0],loc.latlng[1]))   \n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with open('region_coordinates.csv', 'w') as fp:\n",
    "    fp.write('\\n'.join('%s,%f,%f' % x for x in singular_region_complete_with_latlong))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_region_coordinates = pd.read_csv('region_coordinates.csv')\n",
    "\n",
    "df = pd.merge(df, df_region_coordinates, how=\"left\", on=\"region_complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.to_csv(\"competition_dataset/mine_dev.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df = pd.get_dummies(df, columns=[\"designation_tr\",\"province\",\"variety\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df = pd.read_csv(\"competition_dataset/mine_dev.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    df[\"designation_tr\"].value_counts()[:200]\n",
    "    \n",
    "len(df[\"designation_tr\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import gmaps\n",
    "import gmaps.datasets\n",
    "from ipywidgets.embed import embed_minimal_html\n",
    "\n",
    "locations = df[['lat', 'long']]\n",
    "weights = df['quality']\n",
    "fig = gmaps.figure()\n",
    "heatmap_layer = gmaps.heatmap_layer(locations, weights=weights)\n",
    "fig.add_layer(heatmap_layer)\n",
    "embed_minimal_html('export.html', views=[fig])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "df[\"variety\"].value_counts()[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "df[\"province\"].value_counts()[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df[\"region_complete\"] = df[\"region_complete\"].replace(r'^\\s*$', np.NaN, regex=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df = pd.get_dummies(df, columns=[\"region_complete\",\"variety\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.to_csv(\"competition_dataset/mine_dev_with_dummies.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df = pd.read_csv(\"competition_dataset/mine_dev_with_dummies.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Caricamento di x\n",
    "df = pd.read_csv(\"competition_dataset/dev.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_test = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_test[df_test[\"province\"] == \"Idaho\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mask = (df_test['region_1'].isnull()) & (df_test['region_2'].isnull()) & df_test['province'].isnull()\n",
    "\n",
    "df_test[mask].info()\n",
    "\n",
    "# In 13889 record entrambe le region sono nulle\n",
    "# In 3 record entrambe le region sono nulle e anche la province"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mask = df_test['region_1'].isnull() & df_test['region_2'].isnull()\n",
    "\n",
    "province_with_region_nans = df_test[mask][\"province\"].unique()\n",
    "\n",
    "print(len(province_with_region_nans))\n",
    "\n",
    "df_test[df_test[\"province\"] == \"Mendoza Province\"]\n",
    "\n",
    "mask_3 = (df_test['region_1'].isnull()) & (df_test['region_2'].isnull())\n",
    "df_test[mask_3]\n",
    "\n",
    "# In 13889 record entrambe le region sono nulle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mask_2 = (df_test['region_1'].isnull() & df_test['region_2'].isnull()) == False\n",
    "\n",
    "df_test_at_least_one_region = df_test[mask_2]\n",
    "\n",
    "province_with_at_least_one_region = df_test[mask_2][\"province\"].unique()\n",
    "\n",
    "df_test_at_least_one_region[df_test_at_least_one_region[\"province\"].isin(province_with_region_nans)]\n",
    "\n",
    "country_with_at_least_one_region = df_test[mask_2][\"country\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "province_with_at_least_one_region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_test = df.drop_duplicates()\n",
    "\n",
    "mask_2 = (df_test['region_1'].isnull() & df_test['region_2'].isnull()) == False\n",
    "province_with_at_least_one_region = df_test[mask_2][\"province\"].unique()\n",
    "\n",
    "mask4 = (df_test['region_1'].isnull()) & (df_test['region_2'].isnull()) & (df_test['province'].isin(province_with_at_least_one_region))\n",
    "df_test_no_region_in_at_least_one_prov = df_test[mask4].copy() # province che non hanno region in almeno un record e hanno region in almeno un record\n",
    "\n",
    "province_interested = df_test_no_region_in_at_least_one_prov[\"province\"].unique()\n",
    "\n",
    "# In 88 record le due regioni sono nulle e la provincia ha almeno un record con una regione\n",
    "\n",
    "mask = df_test[\"province\"].isin(province_interested)\n",
    "df_test_interested = df_test[mask].copy()\n",
    "\n",
    "df_test_interested.loc[df_test_interested[\"region_1\"] == df_test_interested[\"region_2\"], \"region_2\"] = \"\" # se region1 e 2 sono uguali rendo region 2 una stringa vuota\n",
    "df_test_interested[\"region_complete\"] = df_test_interested[[\"region_1\",\"region_2\"]].fillna(\" \").agg(' '.join, axis=1)\n",
    "df_test_interested[\"region_complete\"] = df_test_interested[\"region_complete\"].replace(r'^\\s*$', np.NaN, regex=True)\n",
    "\n",
    "provinces = (df_test_interested[[\"province\",\"region_complete\"]].groupby(['province']).agg(lambda x:x.value_counts().index[0] if len(x.value_counts().index) > 0 else \"Null\").to_dict())\n",
    "\n",
    "df_test_interested[((df_test_interested[\"region_1\"].isnull() == True) & (df_test_interested[\"region_2\"].isnull() == True))].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mask6 = (df_test['region_1'].isnull()) & (df_test['region_2'].isnull()) & (df_test['country'].isin(country_with_at_least_one_region))\n",
    "df_test_no_region_in_at_least_one_coun = df_test[mask6]\n",
    "\n",
    "df_test_no_region_in_at_least_one_prov.info()\n",
    "\n",
    "# In 88 record le due regioni sono nulle e la nazione ha almeno un record con una regione"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mask5 = (df_test['region_1'].isnull()) & (df_test['region_2'].isnull()) & (df_test['province'].isnull())\n",
    "\n",
    "df_test_no_region_no_prov = df_test[mask5]\n",
    "\n",
    "df_test_no_region_no_prov.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Caricamento di x\n",
    "df = pd.read_csv(\"competition_dataset/dev.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "myFig = plt.figure();\n",
    "ax, bp = df.boxplot(column=['quality'], return_type=\"both\")\n",
    "ax.set_title(\"Boxplot of quality\")\n",
    "myFig.savefig(\"myName.svg\", format=\"svg\")\n",
    "\n",
    "outliers = [flier.get_ydata() for flier in bp[\"fliers\"]]\n",
    "boxes = [box.get_ydata() for box in bp[\"boxes\"]]\n",
    "medians = [median.get_ydata() for median in bp[\"medians\"]]\n",
    "whiskers = [whiskers.get_ydata() for whiskers in bp[\"whiskers\"]]\n",
    "print(outliers, boxes, medians, whiskers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = df.quantile([0.01, 0.25, 0.5, 0.75, 0.99])\n",
    "quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(df)-len(df.drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(df[df[\"quality\"] ==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df.drop_duplicates()\n",
    "\n",
    "df_test[df_test[\"region_1\"] == df_test[\"region_2\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myFig = plt.figure();\n",
    "ax = df_test[\"designation\"].value_counts()[:20][::-1].plot(kind='barh')\n",
    "ax.set_title(\"Top 20 designations in the dataset\")\n",
    "myFig.savefig(\"Top_20_designations.svg\", format=\"svg\",bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outliers = [flier.get_ydata() for flier in bp[\"fliers\"]]\n",
    "sorted(outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"country\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120744\n",
      "85028\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing su x\n",
    "print(len(df))\n",
    "df = df.drop_duplicates() # Rimuovo duplicati\n",
    "print(len(df))\n",
    "\n",
    "\n",
    "df = df.drop(df[df[\"quality\"] == 0].index) #Rimuovo righe con qualità uguale a zero ( sono solo 15 in totale )\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df[\"designation_tr\"] = df[\"designation\"].apply(lambda d: d.lower() if pd.isna(d) == False else d)\n",
    "df[\"designation_tr\"] = df[\"designation_tr\"].replace([\"reserva\",\"riserva\",\"réserve\"],[\"reserve\",\"reserve\",\"reserve\"])\n",
    "\n",
    "\n",
    "df.loc[df[\"region_1\"] == df[\"region_2\"], \"region_2\"] = \"\" # se region1 e 2 sono uguali rendo region 2 una stringa vuota\n",
    "df[\"region_complete\"] = df[[\"region_1\",\"region_2\"]].fillna(\" \").agg(' '.join, axis=1)\n",
    "df[\"region_complete\"] = df[\"region_complete\"].replace(r'^\\s*$', np.NaN, regex=True)\n",
    "\n",
    "df[\"description_len\"] = df[\"description\"].map(len)\n",
    "\n",
    "# --------------------------------------------\n",
    "# Riempio alcune region_complete nulle\n",
    "\n",
    "\n",
    "mask_1 = (df['region_1'].isnull() & df['region_2'].isnull()) == False\n",
    "province_with_at_least_one_region = df[mask_1][\"province\"].unique()\n",
    "\n",
    "mask_2 = (df['region_1'].isnull()) & (df['region_2'].isnull()) & (df['province'].isin(province_with_at_least_one_region))\n",
    "df_no_region_in_at_least_one_prov = df[mask_2].copy() # province che non hanno region in almeno un record e hanno region in almeno un record\n",
    "\n",
    "province_interested = df_no_region_in_at_least_one_prov[\"province\"].unique()\n",
    "\n",
    "# In 88 record le due regioni sono nulle e la provincia ha almeno un record con una regione\n",
    "\n",
    "mask = df[\"province\"].isin(province_interested)\n",
    "df_interested = df[mask].copy()\n",
    "\n",
    "province_region = (df_interested[[\"province\",\"region_complete\"]].groupby(['province']).agg(lambda x:x.value_counts().index[0] if len(x.value_counts().index) > 0 else \"Null\").to_dict())\n",
    "\n",
    "province_region = province_region[\"region_complete\"]\n",
    "\n",
    "def change_region_complete(x):    \n",
    "    return province_region[x[\"province\"]] if (type(x[\"region_complete\"]) == float and math.isnan(x[\"region_complete\"])) else x[\"region_complete\"]\n",
    "\n",
    "df.loc[mask, \"region_complete\"] = df.loc[mask].apply(change_region_complete, axis=1) \n",
    "\n",
    "# --------------------------------------------\n",
    "\n",
    "df[\"region_complete\"].fillna('None', inplace=True)\n",
    "df[\"variety\"].fillna('None', inplace=True)\n",
    "df[\"country\"].fillna('None', inplace=True)\n",
    "df[\"winery\"].fillna('None', inplace=True)\n",
    "df[\"province\"].fillna('None', inplace=True)\n",
    "df[\"designation_tr\"].fillna('None', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.plot.scatter(x='description_len', y='quality')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sample = df.sample(n=1000)\n",
    "sample.plot.scatter(x='description_len', y='quality', alpha=0.7)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"designation_tr\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caricamento di x_eval\n",
    "\n",
    "df_eval = pd.read_csv(\"competition_dataset/eval.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30186 entries, 0 to 30185\n",
      "Data columns (total 8 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   country      30186 non-null  object\n",
      " 1   description  30186 non-null  object\n",
      " 2   designation  20969 non-null  object\n",
      " 3   province     30186 non-null  object\n",
      " 4   region_1     25134 non-null  object\n",
      " 5   region_2     12217 non-null  object\n",
      " 6   variety      30186 non-null  object\n",
      " 7   winery       30186 non-null  object\n",
      "dtypes: object(8)\n",
      "memory usage: 1.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df_eval.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.to_numeric(df['description'],errors='coerce').notna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing su x_eval\n",
    "\n",
    "df_eval[\"designation_tr\"] = df_eval[\"designation\"].apply(lambda d: d.lower() if pd.isna(d) == False else d)\n",
    "df_eval[\"designation_tr\"] = df_eval[\"designation_tr\"].replace([\"reserva\",\"riserva\",\"réserve\"],[\"reserve\",\"reserve\",\"reserve\"])\n",
    "\n",
    "df_eval.loc[df_eval[\"region_1\"] == df_eval[\"region_2\"], \"region_2\"] = \"\"\n",
    "df_eval[\"region_complete\"] = df_eval[[\"region_1\",\"region_2\"]].fillna(\" \").agg(' '.join, axis=1)\n",
    "df_eval[\"region_complete\"] = df_eval[\"region_complete\"].replace(r'^\\s*$', np.NaN, regex=True)\n",
    "\n",
    "df_eval[\"description_len\"] = df_eval[\"description\"].map(len)\n",
    "\n",
    "mask = df_eval[\"province\"].isin(province_interested)\n",
    "df_eval.loc[mask, \"region_complete\"] = df_eval.loc[mask].apply(change_region_complete, axis=1) \n",
    "\n",
    "\n",
    "df_eval[\"region_complete\"].fillna('None', inplace=True)\n",
    "df_eval[\"variety\"].fillna('None', inplace=True)\n",
    "df_eval[\"country\"].fillna('None', inplace=True)\n",
    "df_eval[\"winery\"].fillna('None', inplace=True)\n",
    "df_eval[\"province\"].fillna('None', inplace=True)\n",
    "\n",
    "df_eval[\"designation_tr\"].fillna('None', inplace=True)\n",
    "\n",
    "df_eval[\"designation_tr\"].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Aggiungo a x i bit delle winery con più vini\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "df_group_by_winery = df[[\"winery\",\"quality\"]].groupby(by=\"winery\",sort=True).agg(['mean', 'count']).sort_values(by=(\"quality\",\"count\"),ascending=False)\n",
    "\n",
    "\n",
    "top_N_winery = df_group_by_winery[df_group_by_winery[(\"quality\",\"count\")] >= 20].index.values\n",
    "\n",
    "df.loc[~df[\"winery\"].isin(top_N_winery) ,\"winery\"] = \"None\"\n",
    "\n",
    "ohc_w = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "\n",
    "ohWinery = ohc_w.fit_transform(df[\"winery\"].values.reshape(-1,1)).toarray()\n",
    "dfOneHot_w = pd.DataFrame(ohWinery, columns=[\"winery_\" + str(ohc_w.categories_[0][i]) for i in range(len(ohc_w.categories_[0]))])\n",
    "\n",
    "df = pd.concat([df,dfOneHot_w], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_N_winery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[\"province\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Aggiungo a x_eval i bit delle winery con più vini\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohWinery = ohc_w.transform(df_eval[\"winery\"].values.reshape(-1,1)).toarray()\n",
    "dfOneHot_w = pd.DataFrame(ohWinery, columns=[\"winery_\" + str(ohc_w.categories_[0][i]) for i in range(len(ohc_w.categories_[0]))])\n",
    "\n",
    "df_eval = pd.concat([df_eval,dfOneHot_w], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"quality\"] == 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Aggiungo coordinate lat long a x\n",
    "df_region_coordinates = pd.read_csv('region_coordinates.csv')\n",
    "\n",
    "df = pd.merge(df, df_region_coordinates, how=\"left\", on=\"region_complete\")\n",
    "\n",
    "print(df.columns)\n",
    "lat_mean = df['lat'].mean()\n",
    "long_mean = df['long'].mean()\n",
    "\n",
    "df[\"lat\"].fillna(lat_mean, inplace=True)\n",
    "df[\"long\"].fillna(long_mean, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Aggiungo coordinate lat long a x_eval\n",
    "df_region_coordinates = pd.read_csv('region_coordinates.csv')\n",
    "\n",
    "df_eval = pd.merge(df_eval, df_region_coordinates, how=\"left\", on=\"region_complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Aggiungo a x bit di encoding di region_complete e variety e country e province\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohc_r = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "ohc_v = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "ohc_c = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "ohc_p = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "\n",
    "ohRegion = ohc_r.fit_transform(df[\"region_complete\"].values.reshape(-1,1)).toarray()\n",
    "ohVariety = ohc_v.fit_transform(df[\"variety\"].values.reshape(-1,1)).toarray()\n",
    "ohCountry = ohc_c.fit_transform(df[\"country\"].values.reshape(-1,1)).toarray()\n",
    "ohProvince = ohc_p.fit_transform(df[\"province\"].values.reshape(-1,1)).toarray()\n",
    "\n",
    "dfOneHot_r = pd.DataFrame(ohRegion, columns=[\"region_complete_\" + str(ohc_r.categories_[0][i]) for i in range(len(ohc_r.categories_[0]))])\n",
    "dfOneHot_v = pd.DataFrame(ohVariety, columns=[\"variety_\" + str(ohc_v.categories_[0][i]) for i in range(len(ohc_v.categories_[0]))])\n",
    "dfOneHot_c = pd.DataFrame(ohCountry, columns=[\"country_\" + str(ohc_c.categories_[0][i]) for i in range(len(ohc_c.categories_[0]))])\n",
    "dfOneHot_p = pd.DataFrame(ohProvince, columns=[\"province_\" + str(ohc_p.categories_[0][i]) for i in range(len(ohc_p.categories_[0]))])\n",
    "\n",
    "df = pd.concat([df,dfOneHot_r,dfOneHot_v, dfOneHot_c, dfOneHot_p], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggiungo a x_eval bit di encoding di region_complete e variety e country e province\n",
    "\n",
    "dfOneHot_r = pd.DataFrame(ohRegion, columns=[\"region_complete_\" + str(ohc_r.categories_[0][i]) for i in range(len(ohc_r.categories_[0]))])\n",
    "dfOneHot_v = pd.DataFrame(ohVariety, columns=[\"variety_\" + str(ohc_v.categories_[0][i]) for i in range(len(ohc_v.categories_[0]))])\n",
    "dfOneHot_c = pd.DataFrame(ohCountry, columns=[\"country_\" + str(ohc_c.categories_[0][i]) for i in range(len(ohc_c.categories_[0]))])\n",
    "dfOneHot_p = pd.DataFrame(ohProvince, columns=[\"province_\" + str(ohc_p.categories_[0][i]) for i in range(len(ohc_p.categories_[0]))])\n",
    "\n",
    "df_eval = pd.concat([df_eval,dfOneHot_r,dfOneHot_v,dfOneHot_c,dfOneHot_p], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_group_by_designation_tr = df[[\"designation_tr\",\"quality\"]].groupby(by=\"designation_tr\",sort=True).agg(['mean','count'])\n",
    "df_group_by_designation_tr.reset_index()\n",
    "df_group_by_designation_tr = df_group_by_designation_tr[df_group_by_designation_tr[(\"quality\",\"count\")] >= 10]\n",
    "\n",
    "df_group_by_designation_tr.drop([\"None\"], inplace=True)\n",
    "\n",
    "df_group_by_designation_tr.columns = df_group_by_designation_tr.columns.droplevel(0)\n",
    "df_group_by_designation_tr.drop(columns=[\"count\"], inplace=True)\n",
    "df_group_by_designation_tr.columns = [\"designation_tr_quality_mean\"]\n",
    "\n",
    "df_group_by_designation_tr\n",
    "#df = pd.merge(df, df_group_by_designation_tr, how=\"left\", on=\"designation_tr\")\n",
    "\n",
    "df_test = pd.merge(df, df_group_by_designation_tr, how=\"left\", on=\"designation_tr\")\n",
    "\n",
    "df_test[\"designation_tr_quality_mean\"].fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggiungo a x i chunks delle designation\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "df_group_by_designation_tr = df[[\"designation_tr\",\"quality\"]].groupby(by=\"designation_tr\",sort=True).agg(['mean','count'])\n",
    "df_group_by_designation_tr.reset_index()\n",
    "df_group_by_designation_tr = df_group_by_designation_tr[df_group_by_designation_tr[(\"quality\",\"count\")] >= 10]\n",
    "\n",
    "df_group_by_designation_tr.drop([\"None\"], inplace=True)\n",
    "\n",
    "df_group_by_designation_tr.columns = df_group_by_designation_tr.columns.droplevel(0)\n",
    "df_group_by_designation_tr.drop(columns=[\"count\"], inplace=True)\n",
    "df_group_by_designation_tr.columns = [\"desi_quality_mean\"]\n",
    "\n",
    "\n",
    "df = pd.merge(df, df_group_by_designation_tr, how=\"left\", on=\"designation_tr\")\n",
    "df[\"desi_quality_mean\"].fillna(-1, inplace=True)\n",
    "df[\"desi_quality_mean\"] = df[\"desi_quality_mean\"].astype(int)\n",
    "\n",
    "ohc_d = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "ohDesignation_quality_mean = ohc_d.fit_transform(df[\"desi_quality_mean\"].values.reshape(-1,1)).toarray()\n",
    "\n",
    "dfOneHot_dq = pd.DataFrame(ohDesignation_quality_mean, columns=[\"desi_quality_mean_\" + str(ohc_d.categories_[0][i]) for i in range(len(ohc_d.categories_[0]))])\n",
    "\n",
    "df.drop(columns=[\"desi_quality_mean\"], inplace=True)\n",
    " \n",
    "df = pd.concat([df,dfOneHot_dq], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggiungo a x_eval i chunks delle designation\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "df_eval = pd.merge(df_eval, df_group_by_designation_tr, how=\"left\", on=\"designation_tr\")\n",
    "\n",
    "df_eval[\"desi_quality_mean\"] = df_eval[\"desi_quality_mean\"].fillna(-1)\n",
    "\n",
    "ohDesignation_quality_mean = ohc_d.transform(df_eval[\"desi_quality_mean\"].values.reshape(-1,1)).toarray()\n",
    "\n",
    "dfOneHot_dq = pd.DataFrame(ohDesignation_quality_mean, columns=[\"desi_quality_mean_\" + str(ohc_d.categories_[0][i]) for i in range(len(ohc_d.categories_[0]))])\n",
    "\n",
    "df_eval.drop(columns=[\"desi_quality_mean\"], inplace=True)\n",
    "\n",
    "df_eval = pd.concat([df_eval,dfOneHot_dq], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301\n"
     ]
    }
   ],
   "source": [
    "#Aggiungo i bit dei primi N designation a x\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "N = 300 \n",
    "top_N_designation_tr = list(df[\"designation_tr\"].value_counts()[1:N+1].keys()) #non includo None values\n",
    "\n",
    "df.loc[~df[\"designation_tr\"].isin(top_N_designation_tr) ,\"designation_tr\"] = \"None\"\n",
    "\n",
    "ohc_d = OneHotEncoder(handle_unknown=\"ignore\") \n",
    "ohDesignation = ohc_d.fit_transform(df[\"designation_tr\"].values.reshape(-1,1)).toarray()\n",
    "\n",
    "dfOneHot_d = pd.DataFrame(ohDesignation, columns=[\"designation_tr_\" + str(ohc_d.categories_[0][i]) for i in range(len(ohc_d.categories_[0]))])\n",
    "\n",
    "df = pd.concat([df,dfOneHot_d], axis=1)\n",
    "\n",
    "print(len(ohc_d.categories_[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54832"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eval[\"designation_tr\"].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aggiungo i bit dei primi N designation a x eval\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "df_eval.loc[~df_eval[\"designation_tr\"].isin(top_N_designation_tr) ,\"designation_tr\"] = \"None\"\n",
    "\n",
    "ohDesignation = ohc_d.transform(df_eval[\"designation_tr\"].values.reshape(-1,1)).toarray() \n",
    "\n",
    "dfOneHot_d = pd.DataFrame(ohDesignation, columns=[\"designation_tr_\" + str(ohc_d.categories_[0][i]) for i in range(len(ohc_d.categories_[0]))])\n",
    "\n",
    "\n",
    "df_eval = pd.concat([df_eval,dfOneHot_d], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aggiungo a x i bit delle N parole più frequenti nella description \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\", binary=True, use_idf=False,norm=False)\n",
    "\n",
    "wpm = vectorizer.fit_transform(df[\"description\"].fillna(\"\"))\n",
    "\n",
    "N = 1000\n",
    "freq = sorted(zip(vectorizer.get_feature_names(), wpm.sum(axis=0).tolist()[0]),key=lambda x: x[1], reverse=True)[:N]\n",
    "\n",
    "words = [ word for word, _ in freq ]\n",
    "mask = [ w in words for w in vectorizer.get_feature_names() ]\n",
    "words_ = [ w for w in vectorizer.get_feature_names() if w in words ]\n",
    "df_words = pd.DataFrame(data=wpm[:, np.array(mask)].toarray(),columns=[f\"word_{word}\" for word in words_], index=df.index)\n",
    "\n",
    "df = pd.concat([df,df_words], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aggiungo a x_eval i bit delle N parole più frequenti nella description \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "wpm = vectorizer.transform(df_eval[\"description\"].fillna(\"\"))\n",
    "\n",
    "df_words = pd.DataFrame(data=wpm[:, np.array(mask)].toarray(),columns=[f\"word_{word}\" for word in words_], index=df_eval.index)\n",
    "\n",
    "df_eval = pd.concat([df_eval,df_words], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# TEST Aggiungo a x i bit delle N parole più frequenti nella description \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\", binary=False, use_idf=False)\n",
    "\n",
    "wpm = vectorizer.fit_transform(df[\"description\"].fillna(\"\"))\n",
    "\n",
    "N = 1000\n",
    "freq = sorted(zip(vectorizer.get_feature_names(), wpm.sum(axis=0).tolist()[0]),key=lambda x: x[1], reverse=True)[:N]\n",
    "\n",
    "words = [ word for word, _ in freq ]\n",
    "mask = [ w in words for w in vectorizer.get_feature_names() ]\n",
    "words_ = [ w for w in vectorizer.get_feature_names() if w in words ]\n",
    "df_words = pd.DataFrame(data=wpm[:, np.array(mask)].toarray(),columns=[f\"word_{word}\" for word in words_], index=df.index)\n",
    "\n",
    "df = pd.concat([df,df_words], axis=1)\n",
    "df_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_words[df_words['word_000'].gt(0)].index[0]\n",
    "\n",
    "df_words.loc[74, \"word_000\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST Aggiungo a x_eval i bit delle N parole più frequenti nella description \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "wpm = vectorizer.transform(df_eval[\"description\"].fillna(\"\"))\n",
    "\n",
    "df_words_eval = pd.DataFrame(data=wpm[:, np.array(mask)].toarray(),columns=[f\"word_{word}\" for word in words_], index=df_eval.index)\n",
    "\n",
    "df_eval = pd.concat([df_eval,df_words_eval], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Test tdidf description su x\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "N = 500\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\", use_idf=True, max_features = N)\n",
    "\n",
    "wpm = vectorizer.fit_transform(df[\"description\"].fillna(\"\"))\n",
    "\n",
    "words = [ word for word, _ in freq ]\n",
    "mask = [ w in words for w in vectorizer.get_feature_names() ]\n",
    "words_ = [ w for w in vectorizer.get_feature_names() if w in words ]\n",
    "df_words = pd.DataFrame(data=wpm[:, np.array(mask)].toarray(),columns=[f\"word_{word}\" for word in words_], index=df.index)\n",
    "\n",
    "df_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "regr = RandomForestRegressor(n_estimators=100, n_jobs=-1,max_features=\"sqrt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.iloc[:,9:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Test PCA \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "X = df.iloc[:, 11:]\n",
    "y = df[\"quality\"]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_rescaled = scaler.fit_transform(X)\n",
    "#y_rescaled = scaler.fit_transform(y)\n",
    "\n",
    "pca = PCA(n_components = 0.99)\n",
    "pca.fit(X_rescaled)\n",
    "reduced = pca.transform(X_rescaled)\n",
    "\n",
    "reduced\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reduced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#Predict con PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "            \n",
    "X_train, X_test, y_train, y_test = train_test_split(reduced, y, test_size=0.25, random_state=34)\n",
    "\n",
    "regr.fit(X_train,y_train)\n",
    "            \n",
    "y_pred = regr.predict(X_test)\n",
    "\n",
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5387298957755389"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "X = df.iloc[:, 11:]\n",
    "y = df[\"quality\"]\n",
    "            \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=34)\n",
    "\n",
    "regr.fit(X_train,y_train)\n",
    "            \n",
    "y_pred = regr.predict(X_test)\n",
    "\n",
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# togliendo i duplicati r2 score locale  0.5355624548339075"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('winery_Airfield Estates', 0.0390962160657449),\n",
       " ('region_complete_Capay Valley Central Valley', 0.014496355630152245),\n",
       " ('variety_Portuguese White', 0.012814359845117857),\n",
       " ('region_complete_Offida Passerina  ', 0.007959913745555637),\n",
       " ('country_Hungary', 0.007678241449609029),\n",
       " ('word_term', 0.007494305733826686),\n",
       " ('word_river', 0.006741329494495813),\n",
       " ('province_Casablanca Valley', 0.006180301678523406),\n",
       " ('word_provides', 0.006014057004786955),\n",
       " ('province_Cederberg', 0.005738109435134294),\n",
       " ('word_ample', 0.005308680773053183),\n",
       " ('region_complete_Napa-Sonoma-Monterey North Coast', 0.005084173039026993),\n",
       " ('word_blueberry', 0.0050013262327121425),\n",
       " ('province_Alentejano', 0.004948060667372213),\n",
       " ('word_small', 0.004938909530538018),\n",
       " ('province_Colonia', 0.004787556815504861),\n",
       " ('variety_Cabernet Franc-Cabernet Sauvignon', 0.004687021966083517),\n",
       " ('word_colored', 0.004653626500942398),\n",
       " ('word_waves', 0.004581635937340512),\n",
       " ('province_Montevideo', 0.0044435188795946875),\n",
       " ('word_earth', 0.004365664082840457),\n",
       " ('region_complete_Chianti  ', 0.004350862124436303),\n",
       " ('region_complete_Missouri  ', 0.004329991035129554),\n",
       " ('word_worth', 0.004176975286435653),\n",
       " ('region_complete_Calabria  ', 0.004111705777156229),\n",
       " ('word_food', 0.004110335053772843),\n",
       " ('country_Czech Republic', 0.004027916899878538),\n",
       " ('word_opens', 0.004015760915664817),\n",
       " ('word_coconut', 0.003949442670244258),\n",
       " ('province_Aegean', 0.003943370465567918),\n",
       " ('word_vineyard', 0.0038951184904600835),\n",
       " ('word_emerge', 0.0038936946645466205),\n",
       " ('country_Luxembourg', 0.003877116211711032),\n",
       " ('winery_Ortman Family', 0.003864075507142535),\n",
       " ('variety_Cabernet Sauvignon-Tempranillo', 0.0037577178365693943),\n",
       " ('variety_Scheurebe', 0.003747410851983861),\n",
       " ('word_charred', 0.0037199074977675556),\n",
       " ('word_producer', 0.0033994958122990663),\n",
       " ('word_clean', 0.0033468685033100406),\n",
       " ('word_gets', 0.0033232404249380278),\n",
       " ('word_backbone', 0.003310486683738605),\n",
       " ('word_cedar', 0.003161928785101175),\n",
       " ('word_game', 0.003153419515258882),\n",
       " ('word_makes', 0.0031251253278994267),\n",
       " ('word_rough', 0.003110857513561265),\n",
       " ('word_deliciously', 0.003110852560783859),\n",
       " ('word_citrusy', 0.003031254970693939),\n",
       " ('variety_Nuragus', 0.0029781497359656767),\n",
       " ('word_flavors', 0.0029773756298869967),\n",
       " ('word_creamy', 0.0029691333168391466),\n",
       " ('word_starts', 0.0029496624860112217),\n",
       " ('word_assertive', 0.0029372945579336047),\n",
       " ('designation_tr_bacchus vineyard', 0.002883048850068711),\n",
       " ('designation_tr_boushey vineyard', 0.002875648901347033),\n",
       " ('word_pears', 0.0028211733341122164),\n",
       " ('word_lingers', 0.0027598907189170544),\n",
       " ('word_aging', 0.0027570519283112134),\n",
       " ('region_complete_Beaujolais Blanc  ', 0.0027320571914112224),\n",
       " ('word_bodied', 0.002715834084982184),\n",
       " ('word_displays', 0.002672310554119652),\n",
       " ('word_grassy', 0.002643901937626883),\n",
       " ('word_edges', 0.0026239091885063913),\n",
       " ('word_tannin', 0.002598335801728974),\n",
       " ('word_family', 0.0025886955668227396),\n",
       " ('word_appealing', 0.0024735725648715543),\n",
       " ('province_Victoria', 0.002414572844539176),\n",
       " ('word_offer', 0.0023941607606439122),\n",
       " ('word_restrained', 0.002380814831699259),\n",
       " ('region_complete_St.-Romain  ', 0.0023743570296390908),\n",
       " ('word_sourced', 0.002365801708263572),\n",
       " ('variety_Plavac Mali', 0.00233260081136893),\n",
       " ('word_decade', 0.0023306462478181918),\n",
       " ('province_Colares', 0.002319511305212487),\n",
       " ('word_cranberry', 0.002308559027423745),\n",
       " ('variety_Rufete', 0.0022922646875096165),\n",
       " ('province_Beotia', 0.002283031661451212),\n",
       " ('word_screwcap', 0.002282014254943437),\n",
       " ('word_entry', 0.002267568695269358),\n",
       " ('country_None', 0.0022429944302006493),\n",
       " ('word_floor', 0.0022373005242559835),\n",
       " ('word_blanc', 0.0022230066996528565),\n",
       " ('word_weight', 0.002182130103064063),\n",
       " ('word_chard', 0.002179175883969673),\n",
       " ('word_accent', 0.0021650661114199156),\n",
       " ('word_minerally', 0.0021459154763234655),\n",
       " ('word_rock', 0.0021179909784440677),\n",
       " ('word_suggest', 0.0021146368347573926),\n",
       " ('word_complete', 0.0020971637294433716),\n",
       " ('word_young', 0.002051372835611963),\n",
       " ('word_perfume', 0.0020441336641601414),\n",
       " ('variety_Chardonnay-Sauvignon Blanc', 0.00202573098087587),\n",
       " ('word_slight', 0.002010936860206135),\n",
       " ('variety_Chenin Blanc', 0.0020087280343652615),\n",
       " ('word_benefit', 0.0020046636342275085),\n",
       " ('variety_Rolle', 0.0019994064442858685),\n",
       " ('word_early', 0.0019969379402849418),\n",
       " ('word_particularly', 0.0019902432083054094),\n",
       " ('variety_Roter Traminer', 0.0019790668997776138),\n",
       " ('word_gives', 0.001964075178230776),\n",
       " ('word_flavor', 0.0019433878420185397)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(zip(list(X.columns[11:]), regr.feature_importances_), key=lambda x: x[1],reverse=True)[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'winery': 2,\n",
       "         'region': 8,\n",
       "         'variety': 11,\n",
       "         'country': 4,\n",
       "         'word': 64,\n",
       "         'province': 9,\n",
       "         'designation': 2})"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# features importances\n",
    "import collections\n",
    "\n",
    "feat_importances = sorted(zip(list(X.columns[11:]), regr.feature_importances_), key=lambda x: x[1],reverse=True)[:100]\n",
    "\n",
    "features = [element[0] for element in feat_importances]\n",
    "features = [element.split(\"_\")[0] for element in features]\n",
    "\n",
    "collections.Counter(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Ricerca con grid search\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "X = df.iloc[:, 11:]\n",
    "y = df[\"quality\"]\n",
    "            \n",
    "param_grid = {\n",
    "\"n_estimators\": [100, 250],\n",
    "\"criterion\": [\"mse\"],\n",
    "\"max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
    "\"random_state\": [42], # always use the samet random seed\n",
    "\"n_jobs\": [-1], # for parallelization\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(RandomForestRegressor(), param_grid, scoring=\"r2\", n_jobs=-1,cv=5)\n",
    "gs.fit(X, y)\n",
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import FunctionTransformer, PolynomialFeatures\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "def evaluate_model(X, y, model, model_name):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,train_size=0.25,random_state=42,shuffle=True)\n",
    "    # plot the real function and the training points\n",
    "    LW = 2\n",
    "    #fig, ax = plt.subplots()\n",
    "    #ax.plot(X, y, color='cornflowerblue', linewidth=.5*LW, label=\"ground truth\")\n",
    "    #ax.scatter(X_train, y_train, color='navy', s=30, marker='o',label=\"training points\")\n",
    "    # predict the test points and plot them onto the chart\n",
    "    model.fit(X_train, y_train)\n",
    "    y_hat = model.predict(X_test)\n",
    "    #ax.plot(X_test, y_hat, linewidth=LW, label=name, color='r')\n",
    "    #fig.suptitle(f\"{f} approximated by {model_name}\")\n",
    "    #fig.legend()\n",
    "    return r2_score(y_test, y_hat)\n",
    "\n",
    "\n",
    "\n",
    "degree = 2\n",
    "models = [\n",
    "    LinearRegression(),\n",
    "    Ridge(random_state=42),\n",
    "    RandomForestRegressor(n_estimators=100,n_jobs=-1,max_features=\"sqrt\"),\n",
    "    make_pipeline(\n",
    "        make_column_transformer(\n",
    "            (FunctionTransformer(np.sin), [0]),\n",
    "            (PolynomialFeatures(degree), [0])\n",
    "        ),\n",
    "        LinearRegression()\n",
    "    ),\n",
    "    make_pipeline(\n",
    "        make_column_transformer(\n",
    "            (FunctionTransformer(np.sin), [0]),\n",
    "            (PolynomialFeatures(degree), [0])\n",
    "        ),\n",
    "        Ridge(alpha=1)\n",
    "    )\n",
    "]\n",
    "\n",
    "names = [\n",
    "    'linreg',\n",
    "    'ridge',\n",
    "    'rf',\n",
    "    f'sin+poly{degree}+linreg',\n",
    "    f'sin+poly{degree}+ridge'\n",
    "]\n",
    "\n",
    "\n",
    "t = PrettyTable()\n",
    "t.field_names = ['model', 'R2']\n",
    "X = df.iloc[:, 11:]\n",
    "y = df[\"quality\"]\n",
    "for model, name in zip(models, names):\n",
    "    print(model, name)\n",
    "    r2 = evaluate_model(X, y, model, name)\n",
    "    t.add_row([name, r2])\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "X = df.iloc[:, 11:]\n",
    "y = df[\"quality\"]\n",
    "\n",
    "regr_rf = RandomForestRegressor()\n",
    "parameters_rf = {'n_estimators':[50,100], 'n_jobs':[-1], \"max_features\":[\"sqrt\",\"log2\"], \"random_state\":[34]}\n",
    "clf_rf = GridSearchCV(regr_rf, parameters_rf, scoring=\"r2\",verbose=3)\n",
    "\n",
    "clf_rf.fit(X,y)\n",
    "print(clf_rf.best_params_)\n",
    "print(clf_rf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "X = df.iloc[:, 11:]\n",
    "y = df[\"quality\"]\n",
    "\n",
    "regr_rf = Ridge()\n",
    "parameters_rf = { \"alpha\":np.arange(0.1, 1.1, 0.2), \"fit_intercept\":[True,False],\"random_state\":[34]}\n",
    "clf_rf = GridSearchCV(regr_rf, parameters_rf, scoring=\"r2\",verbose=3)\n",
    "\n",
    "clf_rf.fit(X,y)\n",
    "print(clf_rf.best_params_)\n",
    "print(clf_rf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "X = df.iloc[:, 11:]\n",
    "y = df[\"quality\"]\n",
    "\n",
    "regr_rf = Lasso()\n",
    "parameters_rf = { \"alpha\":np.arange(0.1, 1.1, 0.2), \"fit_intercept\":[True,False],\"random_state\":[34]}\n",
    "clf_rf = GridSearchCV(regr_rf, parameters_rf, scoring=\"r2\",verbose=3)\n",
    "\n",
    "clf_rf.fit(X,y)\n",
    "print(clf_rf.best_params_)\n",
    "print(clf_rf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "freq[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(zip(list(X.columns[11:]), regr.feature_importances_), key=lambda x: x[1],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train[\"lat\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(set(list(df_eval.iloc[:,10:].columns)) - set(list(df.iloc[:, 11:].columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval.drop(columns=[\"desi_quality_mean\"], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval.iloc[:,10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.iloc[:, 11:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "regr = RandomForestRegressor(n_estimators=100, n_jobs=-1,max_features=\"sqrt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, 11:]\n",
    "y = df[\"quality\"]\n",
    "            \n",
    "regr.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_eval = df_eval.iloc[:,10:]\n",
    "\n",
    "y_eval = regr.predict(X_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"my_submission.csv\",\"w\") as f:\n",
    "    f.write(\"Id,Predicted\\n\")\n",
    "    for i,result in enumerate(y_eval):\n",
    "        f.write(str(i) + \",\" + str(y_eval[i])+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df = pd.read_csv(\"competition_dataset/dev.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(df[pd.isna(df[\"region_1\"]) & pd.isna(df[\"region_2\"])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "print(df[\"designation\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    print(df[\"designation\"].value_counts()[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#df[df[\"designation_tr\"] == \"reserva\" | df[\"designation_tr\"] == \"riserva\" | df[\"designation_tr\"] == \"rèserve\"][\"designation_tr\"] = \"reserve\"\n",
    "df[\"designation_tr\"] = df[\"designation\"].apply(lambda d: d.lower() if pd.isna(d) == False else d)\n",
    "df[\"designation_tr\"] = df[\"designation_tr\"].replace([\"reserva\",\"riserva\",\"rèserve\"],[\"reserve\",\"reserve\",\"reserve\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rimuovo regioni doppie e creo regione unica\n",
    "df.loc[df[\"region_1\"] == df[\"region_2\"], \"region_2\"] = \"\"\n",
    "df[\"region_complete\"] = df[[\"region_1\",\"region_2\"]].fillna(\" \").agg(' '.join, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "singular_region_complete = set(df[\"region_complete\"])\n",
    "len(singular_region_complete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import geocoder\n",
    "\n",
    "g = geocoder.arcgis('Redlands, CA')\n",
    "print(g.latlng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "import geocoder\n",
    "\n",
    "singular_region_complete_with_latlong = []\n",
    "for region in singular_region_complete:\n",
    "    print(region)\n",
    "    loc =  geocoder.arcgis(region,maxRows=1)\n",
    "    print(loc)\n",
    "    try:\n",
    "        singular_region_complete_with_latlong.append((region, loc.latlng[0],loc.latlng[1]))   \n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with open('region_coordinates.csv', 'w') as fp:\n",
    "    fp.write('\\n'.join('%s,%f,%f' % x for x in singular_region_complete_with_latlong))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_region_coordinates = pd.read_csv('region_coordinates.csv')\n",
    "\n",
    "df = pd.merge(df, df_region_coordinates, how=\"left\", on=\"region_complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.to_csv(\"competition_dataset/mine_dev.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df = pd.get_dummies(df, columns=[\"designation_tr\",\"province\",\"variety\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df = pd.read_csv(\"competition_dataset/mine_dev.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    df[\"designation_tr\"].value_counts()[:200]\n",
    "    \n",
    "len(df[\"designation_tr\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import gmaps\n",
    "import gmaps.datasets\n",
    "from ipywidgets.embed import embed_minimal_html\n",
    "\n",
    "locations = df[['lat', 'long']]\n",
    "weights = df['quality']\n",
    "fig = gmaps.figure()\n",
    "heatmap_layer = gmaps.heatmap_layer(locations, weights=weights)\n",
    "fig.add_layer(heatmap_layer)\n",
    "embed_minimal_html('export.html', views=[fig])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "df[\"variety\"].value_counts()[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "df[\"province\"].value_counts()[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df[\"region_complete\"] = df[\"region_complete\"].replace(r'^\\s*$', np.NaN, regex=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df = pd.get_dummies(df, columns=[\"region_complete\",\"variety\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.to_csv(\"competition_dataset/mine_dev_with_dummies.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df = pd.read_csv(\"competition_dataset/mine_dev_with_dummies.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Caricamento di x\n",
    "df = pd.read_csv(\"competition_dataset/dev.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_test = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_test[df_test[\"province\"] == \"Idaho\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mask = (df_test['region_1'].isnull()) & (df_test['region_2'].isnull()) & df_test['province'].isnull()\n",
    "\n",
    "df_test[mask].info()\n",
    "\n",
    "# In 13889 record entrambe le region sono nulle\n",
    "# In 3 record entrambe le region sono nulle e anche la province"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mask = df_test['region_1'].isnull() & df_test['region_2'].isnull()\n",
    "\n",
    "province_with_region_nans = df_test[mask][\"province\"].unique()\n",
    "\n",
    "print(len(province_with_region_nans))\n",
    "\n",
    "df_test[df_test[\"province\"] == \"Mendoza Province\"]\n",
    "\n",
    "mask_3 = (df_test['region_1'].isnull()) & (df_test['region_2'].isnull())\n",
    "df_test[mask_3]\n",
    "\n",
    "# In 13889 record entrambe le region sono nulle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mask_2 = (df_test['region_1'].isnull() & df_test['region_2'].isnull()) == False\n",
    "\n",
    "df_test_at_least_one_region = df_test[mask_2]\n",
    "\n",
    "province_with_at_least_one_region = df_test[mask_2][\"province\"].unique()\n",
    "\n",
    "df_test_at_least_one_region[df_test_at_least_one_region[\"province\"].isin(province_with_region_nans)]\n",
    "\n",
    "country_with_at_least_one_region = df_test[mask_2][\"country\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "province_with_at_least_one_region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_test = df.drop_duplicates()\n",
    "\n",
    "mask_2 = (df_test['region_1'].isnull() & df_test['region_2'].isnull()) == False\n",
    "province_with_at_least_one_region = df_test[mask_2][\"province\"].unique()\n",
    "\n",
    "mask4 = (df_test['region_1'].isnull()) & (df_test['region_2'].isnull()) & (df_test['province'].isin(province_with_at_least_one_region))\n",
    "df_test_no_region_in_at_least_one_prov = df_test[mask4].copy() # province che non hanno region in almeno un record e hanno region in almeno un record\n",
    "\n",
    "province_interested = df_test_no_region_in_at_least_one_prov[\"province\"].unique()\n",
    "\n",
    "# In 88 record le due regioni sono nulle e la provincia ha almeno un record con una regione\n",
    "\n",
    "mask = df_test[\"province\"].isin(province_interested)\n",
    "df_test_interested = df_test[mask].copy()\n",
    "\n",
    "df_test_interested.loc[df_test_interested[\"region_1\"] == df_test_interested[\"region_2\"], \"region_2\"] = \"\" # se region1 e 2 sono uguali rendo region 2 una stringa vuota\n",
    "df_test_interested[\"region_complete\"] = df_test_interested[[\"region_1\",\"region_2\"]].fillna(\" \").agg(' '.join, axis=1)\n",
    "df_test_interested[\"region_complete\"] = df_test_interested[\"region_complete\"].replace(r'^\\s*$', np.NaN, regex=True)\n",
    "\n",
    "provinces = (df_test_interested[[\"province\",\"region_complete\"]].groupby(['province']).agg(lambda x:x.value_counts().index[0] if len(x.value_counts().index) > 0 else \"Null\").to_dict())\n",
    "\n",
    "df_test_interested[((df_test_interested[\"region_1\"].isnull() == True) & (df_test_interested[\"region_2\"].isnull() == True))].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mask6 = (df_test['region_1'].isnull()) & (df_test['region_2'].isnull()) & (df_test['country'].isin(country_with_at_least_one_region))\n",
    "df_test_no_region_in_at_least_one_coun = df_test[mask6]\n",
    "\n",
    "df_test_no_region_in_at_least_one_prov.info()\n",
    "\n",
    "# In 88 record le due regioni sono nulle e la nazione ha almeno un record con una regione"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mask5 = (df_test['region_1'].isnull()) & (df_test['region_2'].isnull()) & (df_test['province'].isnull())\n",
    "\n",
    "df_test_no_region_no_prov = df_test[mask5]\n",
    "\n",
    "df_test_no_region_no_prov.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Caricamento di x\n",
    "df = pd.read_csv(\"competition_dataset/dev.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "myFig = plt.figure();\n",
    "ax, bp = df.boxplot(column=['quality'], return_type=\"both\")\n",
    "ax.set_title(\"Boxplot of quality\")\n",
    "myFig.savefig(\"myName.svg\", format=\"svg\")\n",
    "\n",
    "outliers = [flier.get_ydata() for flier in bp[\"fliers\"]]\n",
    "boxes = [box.get_ydata() for box in bp[\"boxes\"]]\n",
    "medians = [median.get_ydata() for median in bp[\"medians\"]]\n",
    "whiskers = [whiskers.get_ydata() for whiskers in bp[\"whiskers\"]]\n",
    "print(outliers, boxes, medians, whiskers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = df.quantile([0.01, 0.25, 0.5, 0.75, 0.99])\n",
    "quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(df)-len(df.drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(df[df[\"quality\"] ==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df.drop_duplicates()\n",
    "\n",
    "df_test[df_test[\"region_1\"] == df_test[\"region_2\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myFig = plt.figure();\n",
    "ax = df_test[\"designation\"].value_counts()[:20][::-1].plot(kind='barh')\n",
    "ax.set_title(\"Top 20 designations in the dataset\")\n",
    "myFig.savefig(\"Top_20_designations.svg\", format=\"svg\",bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outliers = [flier.get_ydata() for flier in bp[\"fliers\"]]\n",
    "sorted(outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"country\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing su x\n",
    "print(len(df))\n",
    "df = df.drop_duplicates() # Rimuovo duplicati\n",
    "print(len(df))\n",
    "\n",
    "\n",
    "df = df.drop(df[df[\"quality\"] == 0].index) #Rimuovo righe con qualità uguale a zero ( sono solo 15 in totale )\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df[\"designation_tr\"] = df[\"designation\"].apply(lambda d: d.lower() if pd.isna(d) == False else d)\n",
    "df[\"designation_tr\"] = df[\"designation_tr\"].replace([\"reserva\",\"riserva\",\"réserve\"],[\"reserve\",\"reserve\",\"reserve\"])\n",
    "\n",
    "\n",
    "df.loc[df[\"region_1\"] == df[\"region_2\"], \"region_2\"] = \"\" # se region1 e 2 sono uguali rendo region 2 una stringa vuota\n",
    "df[\"region_complete\"] = df[[\"region_1\",\"region_2\"]].fillna(\" \").agg(' '.join, axis=1)\n",
    "df[\"region_complete\"] = df[\"region_complete\"].replace(r'^\\s*$', np.NaN, regex=True)\n",
    "\n",
    "df[\"description_len\"] = df[\"description\"].map(len)\n",
    "\n",
    "# --------------------------------------------\n",
    "# Riempio alcune region_complete nulle\n",
    "\n",
    "\n",
    "mask_1 = (df['region_1'].isnull() & df['region_2'].isnull()) == False\n",
    "province_with_at_least_one_region = df[mask_1][\"province\"].unique()\n",
    "\n",
    "mask_2 = (df['region_1'].isnull()) & (df['region_2'].isnull()) & (df['province'].isin(province_with_at_least_one_region))\n",
    "df_no_region_in_at_least_one_prov = df[mask_2].copy() # province che non hanno region in almeno un record e hanno region in almeno un record\n",
    "\n",
    "province_interested = df_no_region_in_at_least_one_prov[\"province\"].unique()\n",
    "\n",
    "# In 88 record le due regioni sono nulle e la provincia ha almeno un record con una regione\n",
    "\n",
    "mask = df[\"province\"].isin(province_interested)\n",
    "df_interested = df[mask].copy()\n",
    "\n",
    "province_region = (df_interested[[\"province\",\"region_complete\"]].groupby(['province']).agg(lambda x:x.value_counts().index[0] if len(x.value_counts().index) > 0 else \"Null\").to_dict())\n",
    "\n",
    "province_region = province_region[\"region_complete\"]\n",
    "\n",
    "def change_region_complete(x):    \n",
    "    return province_region[x[\"province\"]] if (type(x[\"region_complete\"]) == float and math.isnan(x[\"region_complete\"])) else x[\"region_complete\"]\n",
    "\n",
    "df.loc[mask, \"region_complete\"] = df.loc[mask].apply(change_region_complete, axis=1) \n",
    "\n",
    "# --------------------------------------------\n",
    "\n",
    "df[\"region_complete\"].fillna('None', inplace=True)\n",
    "df[\"variety\"].fillna('None', inplace=True)\n",
    "df[\"country\"].fillna('None', inplace=True)\n",
    "df[\"winery\"].fillna('None', inplace=True)\n",
    "df[\"province\"].fillna('None', inplace=True)\n",
    "df[\"designation_tr\"].fillna('None', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.plot.scatter(x='description_len', y='quality')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sample = df.sample(n=1000)\n",
    "sample.plot.scatter(x='description_len', y='quality', alpha=0.7)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"designation_tr\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caricamento di x_eval\n",
    "\n",
    "df_eval = pd.read_csv(\"competition_dataset/eval.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing su x_eval\n",
    "\n",
    "df_eval[\"designation_tr\"] = df_eval[\"designation\"].apply(lambda d: d.lower() if pd.isna(d) == False else d)\n",
    "df_eval[\"designation_tr\"] = df_eval[\"designation_tr\"].replace([\"reserva\",\"riserva\",\"réserve\"],[\"reserve\",\"reserve\",\"reserve\"])\n",
    "\n",
    "df_eval.loc[df_eval[\"region_1\"] == df_eval[\"region_2\"], \"region_2\"] = \"\"\n",
    "df_eval[\"region_complete\"] = df_eval[[\"region_1\",\"region_2\"]].fillna(\" \").agg(' '.join, axis=1)\n",
    "df_eval[\"region_complete\"] = df_eval[\"region_complete\"].replace(r'^\\s*$', np.NaN, regex=True)\n",
    "\n",
    "df_eval[\"description_len\"] = df_eval[\"description\"].map(len)\n",
    "\n",
    "mask = df_eval[\"province\"].isin(province_interested)\n",
    "df_eval.loc[mask, \"region_complete\"] = df_eval.loc[mask].apply(change_region_complete, axis=1) \n",
    "\n",
    "\n",
    "df_eval[\"region_complete\"].fillna('None', inplace=True)\n",
    "df_eval[\"variety\"].fillna('None', inplace=True)\n",
    "df_eval[\"country\"].fillna('None', inplace=True)\n",
    "df_eval[\"winery\"].fillna('None', inplace=True)\n",
    "df_eval[\"province\"].fillna('None', inplace=True)\n",
    "df_eval[\"designation_tr\"].fillna('None', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Aggiungo a x i bit delle winery con più vini\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "df_group_by_winery = df[[\"winery\",\"quality\"]].groupby(by=\"winery\",sort=True).agg(['mean', 'count']).sort_values(by=(\"quality\",\"count\"),ascending=False)\n",
    "\n",
    "\n",
    "top_N_winery = df_group_by_winery[df_group_by_winery[(\"quality\",\"count\")] >= 20].index.values\n",
    "\n",
    "df.loc[~df[\"winery\"].isin(top_N_winery) ,\"winery\"] = \"None\"\n",
    "\n",
    "ohc_w = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "\n",
    "ohWinery = ohc_w.fit_transform(df[\"winery\"].values.reshape(-1,1)).toarray()\n",
    "dfOneHot_w = pd.DataFrame(ohWinery, columns=[\"winery_\" + str(ohc_w.categories_[0][i]) for i in range(len(ohc_w.categories_[0]))])\n",
    "\n",
    "df = pd.concat([df,dfOneHot_w], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_N_winery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[\"province\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Aggiungo a x_eval i bit delle winery con più vini\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohWinery = ohc_w.transform(df_eval[\"winery\"].values.reshape(-1,1)).toarray()\n",
    "dfOneHot_w = pd.DataFrame(ohWinery, columns=[\"winery_\" + str(ohc_w.categories_[0][i]) for i in range(len(ohc_w.categories_[0]))])\n",
    "\n",
    "df_eval = pd.concat([df_eval,dfOneHot_w], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"quality\"] == 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Aggiungo coordinate lat long a x\n",
    "df_region_coordinates = pd.read_csv('region_coordinates.csv')\n",
    "\n",
    "df = pd.merge(df, df_region_coordinates, how=\"left\", on=\"region_complete\")\n",
    "\n",
    "print(df.columns)\n",
    "lat_mean = df['lat'].mean()\n",
    "long_mean = df['long'].mean()\n",
    "\n",
    "df[\"lat\"].fillna(lat_mean, inplace=True)\n",
    "df[\"long\"].fillna(long_mean, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Aggiungo coordinate lat long a x_eval\n",
    "df_region_coordinates = pd.read_csv('region_coordinates.csv')\n",
    "\n",
    "df_eval = pd.merge(df_eval, df_region_coordinates, how=\"left\", on=\"region_complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix, hstack\n",
    "\n",
    "# Aggiungo a x bit di encoding di region_complete e variety e country e province\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohc_r = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "ohc_v = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "ohc_c = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "ohc_p = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "\n",
    "dfOneHot_r = pd.DataFrame(ohRegion, columns=[\"region_complete_\" + str(ohc_r.categories_[0][i]) for i in range(len(ohc_r.categories_[0]))])\n",
    "dfOneHot_v = pd.DataFrame(ohVariety, columns=[\"variety_\" + str(ohc_v.categories_[0][i]) for i in range(len(ohc_v.categories_[0]))])\n",
    "dfOneHot_c = pd.DataFrame(ohCountry, columns=[\"country_\" + str(ohc_c.categories_[0][i]) for i in range(len(ohc_c.categories_[0]))])\n",
    "dfOneHot_p = pd.DataFrame(ohProvince, columns=[\"province_\" + str(ohc_p.categories_[0][i]) for i in range(len(ohc_p.categories_[0]))])\n",
    "\n",
    "df = pd.concat([df,dfOneHot_r,dfOneHot_v, dfOneHot_c, dfOneHot_p], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggiungo a x_eval bit di encoding di region_complete e variety e country e province\n",
    "\n",
    "dfOneHot_r = pd.DataFrame(ohRegion, columns=[\"region_complete_\" + str(ohc_r.categories_[0][i]) for i in range(len(ohc_r.categories_[0]))])\n",
    "dfOneHot_v = pd.DataFrame(ohVariety, columns=[\"variety_\" + str(ohc_v.categories_[0][i]) for i in range(len(ohc_v.categories_[0]))])\n",
    "dfOneHot_c = pd.DataFrame(ohCountry, columns=[\"country_\" + str(ohc_c.categories_[0][i]) for i in range(len(ohc_c.categories_[0]))])\n",
    "dfOneHot_p = pd.DataFrame(ohProvince, columns=[\"province_\" + str(ohc_p.categories_[0][i]) for i in range(len(ohc_p.categories_[0]))])\n",
    "\n",
    "df_eval = pd.concat([df_eval,dfOneHot_r,dfOneHot_v,dfOneHot_c,dfOneHot_p], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_group_by_designation_tr = df[[\"designation_tr\",\"quality\"]].groupby(by=\"designation_tr\",sort=True).agg(['mean','count'])\n",
    "df_group_by_designation_tr.reset_index()\n",
    "df_group_by_designation_tr = df_group_by_designation_tr[df_group_by_designation_tr[(\"quality\",\"count\")] >= 10]\n",
    "\n",
    "df_group_by_designation_tr.drop([\"None\"], inplace=True)\n",
    "\n",
    "df_group_by_designation_tr.columns = df_group_by_designation_tr.columns.droplevel(0)\n",
    "df_group_by_designation_tr.drop(columns=[\"count\"], inplace=True)\n",
    "df_group_by_designation_tr.columns = [\"designation_tr_quality_mean\"]\n",
    "\n",
    "df_group_by_designation_tr\n",
    "#df = pd.merge(df, df_group_by_designation_tr, how=\"left\", on=\"designation_tr\")\n",
    "\n",
    "df_test = pd.merge(df, df_group_by_designation_tr, how=\"left\", on=\"designation_tr\")\n",
    "\n",
    "df_test[\"designation_tr_quality_mean\"].fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggiungo a x i chunks delle designation\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "df_group_by_designation_tr = df[[\"designation_tr\",\"quality\"]].groupby(by=\"designation_tr\",sort=True).agg(['mean','count'])\n",
    "df_group_by_designation_tr.reset_index()\n",
    "df_group_by_designation_tr = df_group_by_designation_tr[df_group_by_designation_tr[(\"quality\",\"count\")] >= 10]\n",
    "\n",
    "df_group_by_designation_tr.drop([\"None\"], inplace=True)\n",
    "\n",
    "df_group_by_designation_tr.columns = df_group_by_designation_tr.columns.droplevel(0)\n",
    "df_group_by_designation_tr.drop(columns=[\"count\"], inplace=True)\n",
    "df_group_by_designation_tr.columns = [\"desi_quality_mean\"]\n",
    "\n",
    "\n",
    "df = pd.merge(df, df_group_by_designation_tr, how=\"left\", on=\"designation_tr\")\n",
    "df[\"desi_quality_mean\"].fillna(-1, inplace=True)\n",
    "df[\"desi_quality_mean\"] = df[\"desi_quality_mean\"].astype(int)\n",
    "\n",
    "ohc_d = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "ohDesignation_quality_mean = ohc_d.fit_transform(df[\"desi_quality_mean\"].values.reshape(-1,1)).toarray()\n",
    "\n",
    "dfOneHot_dq = pd.DataFrame(ohDesignation_quality_mean, columns=[\"desi_quality_mean_\" + str(ohc_d.categories_[0][i]) for i in range(len(ohc_d.categories_[0]))])\n",
    "\n",
    "df.drop(columns=[\"desi_quality_mean\"], inplace=True)\n",
    " \n",
    "df = pd.concat([df,dfOneHot_dq], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggiungo a x_eval i chunks delle designation\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "df_eval = pd.merge(df_eval, df_group_by_designation_tr, how=\"left\", on=\"designation_tr\")\n",
    "\n",
    "df_eval[\"desi_quality_mean\"] = df_eval[\"desi_quality_mean\"].fillna(-1)\n",
    "\n",
    "ohDesignation_quality_mean = ohc_d.transform(df_eval[\"desi_quality_mean\"].values.reshape(-1,1)).toarray()\n",
    "\n",
    "dfOneHot_dq = pd.DataFrame(ohDesignation_quality_mean, columns=[\"desi_quality_mean_\" + str(ohc_d.categories_[0][i]) for i in range(len(ohc_d.categories_[0]))])\n",
    "\n",
    "df_eval.drop(columns=[\"desi_quality_mean\"], inplace=True)\n",
    "\n",
    "df_eval = pd.concat([df_eval,dfOneHot_dq], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aggiungo i bit dei primi N designation a x\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "N = 300 \n",
    "top_N_designation_tr = list(df[\"designation_tr\"].value_counts()[1:N+1].keys()) #non includo None values\n",
    "\n",
    "df.loc[~df[\"designation_tr\"].isin(top_N_designation_tr) ,\"designation_tr\"] = \"None\"\n",
    "\n",
    "ohc_d = OneHotEncoder(handle_unknown=\"ignore\") \n",
    "ohDesignation_tr = ohc_d.fit_transform(df[\"designation_tr\"].values.reshape(-1,1)).toarray()\n",
    "\n",
    "dfOneHot_d = pd.DataFrame(ohDesignation_tr, columns=[\"designationtr\" + str(ohcd.categories[0][i]) for i in range(len(ohcd.categories[0]))])\n",
    "\n",
    "df = pd.concat([df,dfOneHot_d], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aggiungo i bit dei primi N designation a x eval\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohDesignation = ohc_d.transform(df_eval[\"designation_tr\"].values.reshape(-1,1)).toarray() \n",
    "dfOneHot_d = pd.DataFrame(ohDesignation, columns=[\"designationtr\" + str(ohcd.categories[0][i]) for i in range(len(ohcd.categories[0]))])\n",
    "\n",
    "df_eval = pd.concat([df_eval,dfOneHot_d], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aggiungo a x i bit delle N parole più frequenti nella description \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\", binary=True, use_idf=False,norm=False)\n",
    "\n",
    "wpm = vectorizer.fit_transform(df[\"description\"].fillna(\"\"))\n",
    "\n",
    "N = 1000\n",
    "freq = sorted(zip(vectorizer.get_feature_names(), wpm.sum(axis=0).tolist()[0]),key=lambda x: x[1], reverse=True)[:N]\n",
    "\n",
    "words = [ word for word, _ in freq ]\n",
    "mask = [ w in words for w in vectorizer.get_feature_names() ]\n",
    "words_ = [ w for w in vectorizer.get_feature_names() if w in words ]\n",
    "df_words = pd.DataFrame(data=wpm[:, np.array(mask)].toarray(),columns=[f\"word_{word}\" for word in words_], index=df.index)\n",
    "\n",
    "df = pd.concat([df,df_words], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aggiungo a x_eval i bit delle N parole più frequenti nella description \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "wpm = vectorizer.transform(df_eval[\"description\"].fillna(\"\"))\n",
    "\n",
    "df_words = pd.DataFrame(data=wpm[:, np.array(mask)].toarray(),columns=[f\"word_{word}\" for word in words_], index=df_eval.index)\n",
    "\n",
    "df_eval = pd.concat([df_eval,df_words_eval], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# TEST Aggiungo a x i bit delle N parole più frequenti nella description \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\", binary=False, use_idf=False)\n",
    "\n",
    "wpm = vectorizer.fit_transform(df[\"description\"].fillna(\"\"))\n",
    "\n",
    "N = 1000\n",
    "freq = sorted(zip(vectorizer.get_feature_names(), wpm.sum(axis=0).tolist()[0]),key=lambda x: x[1], reverse=True)[:N]\n",
    "\n",
    "words = [ word for word, _ in freq ]\n",
    "mask = [ w in words for w in vectorizer.get_feature_names() ]\n",
    "words_ = [ w for w in vectorizer.get_feature_names() if w in words ]\n",
    "df_words = pd.DataFrame(data=wpm[:, np.array(mask)].toarray(),columns=[f\"word_{word}\" for word in words_], index=df.index)\n",
    "\n",
    "df = pd.concat([df,df_words], axis=1)\n",
    "df_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_words[df_words['word_000'].gt(0)].index[0]\n",
    "\n",
    "df_words.loc[74, \"word_000\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST Aggiungo a x_eval i bit delle N parole più frequenti nella description \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "wpm = vectorizer.transform(df_eval[\"description\"].fillna(\"\"))\n",
    "\n",
    "df_words_eval = pd.DataFrame(data=wpm[:, np.array(mask)].toarray(),columns=[f\"word_{word}\" for word in words_], index=df_eval.index)\n",
    "\n",
    "df_eval = pd.concat([df_eval,df_words_eval], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Test tdidf description su x\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "N = 500\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\", use_idf=True, max_features = N)\n",
    "\n",
    "wpm = vectorizer.fit_transform(df[\"description\"].fillna(\"\"))\n",
    "\n",
    "words = [ word for word, _ in freq ]\n",
    "mask = [ w in words for w in vectorizer.get_feature_names() ]\n",
    "words_ = [ w for w in vectorizer.get_feature_names() if w in words ]\n",
    "df_words = pd.DataFrame(data=wpm[:, np.array(mask)].toarray(),columns=[f\"word_{word}\" for word in words_], index=df.index)\n",
    "\n",
    "df_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "regr = RandomForestRegressor(n_estimators=100, n_jobs=-1,max_features=\"sqrt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.iloc[:,9:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Test PCA \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "X = df.iloc[:, 11:]\n",
    "y = df[\"quality\"]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_rescaled = scaler.fit_transform(X)\n",
    "#y_rescaled = scaler.fit_transform(y)\n",
    "\n",
    "pca = PCA(n_components = 0.99)\n",
    "pca.fit(X_rescaled)\n",
    "reduced = pca.transform(X_rescaled)\n",
    "\n",
    "reduced\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reduced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#Predict con PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "            \n",
    "X_train, X_test, y_train, y_test = train_test_split(reduced, y, test_size=0.25, random_state=34)\n",
    "\n",
    "regr.fit(X_train,y_train)\n",
    "            \n",
    "y_pred = regr.predict(X_test)\n",
    "\n",
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "X = df.iloc[:, 11:]\n",
    "y = df[\"quality\"]\n",
    "            \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=34)\n",
    "\n",
    "regr.fit(X_train,y_train)\n",
    "            \n",
    "y_pred = regr.predict(X_test)\n",
    "\n",
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# togliendo i duplicati r2 score locale  0.5355624548339075"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features importances\n",
    "import collections\n",
    "\n",
    "feat_importances = sorted(zip(list(X.columns[11:]), regr.feature_importances_), key=lambda x: x[1],reverse=True)[:100]\n",
    "\n",
    "features = [element[0] for element in feat_importances]\n",
    "features = [element.split(\"_\")[0] for element in features]\n",
    "\n",
    "collections.Counter(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Ricerca con grid search\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "X = df.iloc[:, 11:]\n",
    "y = df[\"quality\"]\n",
    "            \n",
    "param_grid = {\n",
    "\"n_estimators\": [100, 250],\n",
    "\"criterion\": [\"mse\"],\n",
    "\"max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
    "\"random_state\": [42], # always use the samet random seed\n",
    "\"n_jobs\": [-1], # for parallelization\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(RandomForestRegressor(), param_grid, scoring=\"r2\", n_jobs=-1,cv=5)\n",
    "gs.fit(X, y)\n",
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import FunctionTransformer, PolynomialFeatures\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "def evaluate_model(X, y, model, model_name):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,train_size=0.25,random_state=42,shuffle=True)\n",
    "    # plot the real function and the training points\n",
    "    LW = 2\n",
    "    #fig, ax = plt.subplots()\n",
    "    #ax.plot(X, y, color='cornflowerblue', linewidth=.5*LW, label=\"ground truth\")\n",
    "    #ax.scatter(X_train, y_train, color='navy', s=30, marker='o',label=\"training points\")\n",
    "    # predict the test points and plot them onto the chart\n",
    "    model.fit(X_train, y_train)\n",
    "    y_hat = model.predict(X_test)\n",
    "    #ax.plot(X_test, y_hat, linewidth=LW, label=name, color='r')\n",
    "    #fig.suptitle(f\"{f} approximated by {model_name}\")\n",
    "    #fig.legend()\n",
    "    return r2_score(y_test, y_hat)\n",
    "\n",
    "\n",
    "\n",
    "degree = 2\n",
    "models = [\n",
    "    LinearRegression(),\n",
    "    Ridge(random_state=42),\n",
    "    RandomForestRegressor(n_estimators=100,n_jobs=-1,max_features=\"sqrt\"),\n",
    "    make_pipeline(\n",
    "        make_column_transformer(\n",
    "            (FunctionTransformer(np.sin), [0]),\n",
    "            (PolynomialFeatures(degree), [0])\n",
    "        ),\n",
    "        LinearRegression()\n",
    "    ),\n",
    "    make_pipeline(\n",
    "        make_column_transformer(\n",
    "            (FunctionTransformer(np.sin), [0]),\n",
    "            (PolynomialFeatures(degree), [0])\n",
    "        ),\n",
    "        Ridge(alpha=1)\n",
    "    )\n",
    "]\n",
    "\n",
    "names = [\n",
    "    'linreg',\n",
    "    'ridge',\n",
    "    'rf',\n",
    "    f'sin+poly{degree}+linreg',\n",
    "    f'sin+poly{degree}+ridge'\n",
    "]\n",
    "\n",
    "\n",
    "t = PrettyTable()\n",
    "t.field_names = ['model', 'R2']\n",
    "X = df.iloc[:, 11:]\n",
    "y = df[\"quality\"]\n",
    "for model, name in zip(models, names):\n",
    "    print(model, name)\n",
    "    r2 = evaluate_model(X, y, model, name)\n",
    "    t.add_row([name, r2])\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "X = df.iloc[:, 11:]\n",
    "y = df[\"quality\"]\n",
    "\n",
    "regr_rf = RandomForestRegressor()\n",
    "parameters_rf = {'n_estimators':[50,100], 'n_jobs':[-1], \"max_features\":[\"sqrt\",\"log2\"], \"random_state\":[34]}\n",
    "clf_rf = GridSearchCV(regr_rf, parameters_rf, scoring=\"r2\",verbose=3)\n",
    "\n",
    "clf_rf.fit(X,y)\n",
    "print(clf_rf.best_params_)\n",
    "print(clf_rf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "X = df.iloc[:, 11:]\n",
    "y = df[\"quality\"]\n",
    "\n",
    "regr_rf = Ridge()\n",
    "parameters_rf = { \"alpha\":np.arange(0.1, 1.1, 0.2), \"fit_intercept\":[True,False],\"random_state\":[34]}\n",
    "clf_rf = GridSearchCV(regr_rf, parameters_rf, scoring=\"r2\",verbose=3)\n",
    "\n",
    "clf_rf.fit(X,y)\n",
    "print(clf_rf.best_params_)\n",
    "print(clf_rf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "X = df.iloc[:, 11:]\n",
    "y = df[\"quality\"]\n",
    "\n",
    "regr_rf = Lasso()\n",
    "parameters_rf = { \"alpha\":np.arange(0.1, 1.1, 0.2), \"fit_intercept\":[True,False],\"random_state\":[34]}\n",
    "clf_rf = GridSearchCV(regr_rf, parameters_rf, scoring=\"r2\",verbose=3)\n",
    "\n",
    "clf_rf.fit(X,y)\n",
    "print(clf_rf.best_params_)\n",
    "print(clf_rf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "freq[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(zip(list(X.columns[11:]), regr.feature_importances_), key=lambda x: x[1],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train[\"lat\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(set(list(df_eval.iloc[:,10:].columns)) - set(list(df.iloc[:, 11:].columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval.drop(columns=[\"desi_quality_mean\"], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval.iloc[:,10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.iloc[:, 11:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "regr = RandomForestRegressor(n_estimators=100, n_jobs=-1,max_features=\"sqrt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, 11:]\n",
    "y = df[\"quality\"]\n",
    "            \n",
    "regr.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_eval = df_eval.iloc[:,10:]\n",
    "\n",
    "y_eval = regr.predict(X_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"my_submission.csv\",\"w\") as f:\n",
    "    f.write(\"Id,Predicted\\n\")\n",
    "    for i,result in enumerate(y_eval):\n",
    "        f.write(str(i) + \",\" + str(y_eval[i])+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}